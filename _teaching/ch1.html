<!DOCTYPE html>

<html>

<head>

<meta charset="utf-8" />
<meta name="generator" content="pandoc" />
<meta http-equiv="X-UA-Compatible" content="IE=EDGE" />


<meta name="author" content="John Doe" />

<meta name="date" content="2005-03-22" />

<title>Habits</title>

<script src="site_libs/header-attrs-2.8/header-attrs.js"></script>
<script src="site_libs/jquery-1.11.3/jquery.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="site_libs/bootstrap-3.3.5/css/bootstrap.min.css" rel="stylesheet" />
<script src="site_libs/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/respond.min.js"></script>
<style>h1 {font-size: 34px;}
       h1.title {font-size: 38px;}
       h2 {font-size: 30px;}
       h3 {font-size: 24px;}
       h4 {font-size: 18px;}
       h5 {font-size: 16px;}
       h6 {font-size: 12px;}
       code {color: inherit; background-color: rgba(0, 0, 0, 0.04);}
       pre:not([class]) { background-color: white }</style>
<script src="site_libs/navigation-1.1/tabsets.js"></script>
<link href="site_libs/highlightjs-9.12.0/textmate.css" rel="stylesheet" />
<script src="site_libs/highlightjs-9.12.0/highlight.js"></script>

<style type="text/css">
  code{white-space: pre-wrap;}
  span.smallcaps{font-variant: small-caps;}
  span.underline{text-decoration: underline;}
  div.column{display: inline-block; vertical-align: top; width: 50%;}
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
  ul.task-list{list-style: none;}
    </style>

<style type="text/css">code{white-space: pre;}</style>
<script type="text/javascript">
if (window.hljs) {
  hljs.configure({languages: []});
  hljs.initHighlightingOnLoad();
  if (document.readyState && document.readyState === "complete") {
    window.setTimeout(function() { hljs.initHighlighting(); }, 0);
  }
}
</script>








<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
img {
  max-width:100%;
}
.tabbed-pane {
  padding-top: 12px;
}
.html-widget {
  margin-bottom: 20px;
}
button.code-folding-btn:focus {
  outline: none;
}
summary {
  display: list-item;
}
pre code {
  padding: 0;
}
</style>


<style type="text/css">
.dropdown-submenu {
  position: relative;
}
.dropdown-submenu>.dropdown-menu {
  top: 0;
  left: 100%;
  margin-top: -6px;
  margin-left: -1px;
  border-radius: 0 6px 6px 6px;
}
.dropdown-submenu:hover>.dropdown-menu {
  display: block;
}
.dropdown-submenu>a:after {
  display: block;
  content: " ";
  float: right;
  width: 0;
  height: 0;
  border-color: transparent;
  border-style: solid;
  border-width: 5px 0 5px 5px;
  border-left-color: #cccccc;
  margin-top: 5px;
  margin-right: -10px;
}
.dropdown-submenu:hover>a:after {
  border-left-color: #adb5bd;
}
.dropdown-submenu.pull-left {
  float: none;
}
.dropdown-submenu.pull-left>.dropdown-menu {
  left: -100%;
  margin-left: 10px;
  border-radius: 6px 0 6px 6px;
}
</style>

<script type="text/javascript">
// manage active state of menu based on current page
$(document).ready(function () {
  // active menu anchor
  href = window.location.pathname
  href = href.substr(href.lastIndexOf('/') + 1)
  if (href === "")
    href = "index.html";
  var menuAnchor = $('a[href="' + href + '"]');

  // mark it active
  menuAnchor.tab('show');

  // if it's got a parent navbar menu mark it active as well
  menuAnchor.closest('li.dropdown').addClass('active');

  // Navbar adjustments
  var navHeight = $(".navbar").first().height() + 15;
  var style = document.createElement('style');
  var pt = "padding-top: " + navHeight + "px; ";
  var mt = "margin-top: -" + navHeight + "px; ";
  var css = "";
  // offset scroll position for anchor links (for fixed navbar)
  for (var i = 1; i <= 6; i++) {
    css += ".section h" + i + "{ " + pt + mt + "}\n";
  }
  style.innerHTML = "body {" + pt + "padding-bottom: 40px; }\n" + css;
  document.head.appendChild(style);
});
</script>

<!-- tabsets -->

<style type="text/css">
.tabset-dropdown > .nav-tabs {
  display: inline-table;
  max-height: 500px;
  min-height: 44px;
  overflow-y: auto;
  border: 1px solid #ddd;
  border-radius: 4px;
}

.tabset-dropdown > .nav-tabs > li.active:before {
  content: "";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li.active:before {
  content: "&#xe258;";
  border: none;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open:before {
  content: "";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs > li.active {
  display: block;
}

.tabset-dropdown > .nav-tabs > li > a,
.tabset-dropdown > .nav-tabs > li > a:focus,
.tabset-dropdown > .nav-tabs > li > a:hover {
  border: none;
  display: inline-block;
  border-radius: 4px;
  background-color: transparent;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li {
  display: block;
  float: none;
}

.tabset-dropdown > .nav-tabs > li {
  display: none;
}
</style>

<!-- code folding -->




</head>

<body>


<div class="container-fluid main-container">




<div class="navbar navbar-default  navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="index.html">MC</a>
    </div>
    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
        <li>
  <a href="index.html">Home</a>
</li>
<li>
  <a href="teaching.html">Teaching</a>
</li>
<li>
  <a href="ressources.html">Ressources</a>
</li>
<li>
  <a href="cv.html">cv</a>
</li>
      </ul>
      <ul class="nav navbar-nav navbar-right">
        
      </ul>
    </div><!--/.nav-collapse -->
  </div><!--/.container -->
</div><!--/.navbar -->

<div id="header">



<h1 class="title toc-ignore">Habits</h1>
<h4 class="author">John Doe</h4>
<h4 class="date">March 22, 2005</h4>

</div>

<div id="TOC">
<ul>
<li><a href="#introduction---roadmap-for-this-semester">1. Introduction - Roadmap for this semester</a></li>
<li><a href="#glm-for-non-normal-responses">2. GLM for non-normal responses</a>
<ul>
<li><a href="#binary-models">Binary models</a>
<ul>
<li><a href="#linear-probability-model">Linear probability model</a></li>
<li><a href="#linkresponse-functions">Link/response functions</a></li>
<li><a href="#response-functions">Response functions</a></li>
<li><a href="#latent-utility-model">Latent utility model</a></li>
<li><a href="#interpretation-of-parameters-in-the-logit-model">Interpretation of parameters in the logit model</a></li>
<li><a href="#grouped-data">Grouped data</a></li>
<li><a href="#overdispersion">Overdispersion</a></li>
<li><a href="#maximum-likelihood-for-binary-glm">Maximum Likelihood for binary GLM</a></li>
</ul></li>
<li><a href="#count-regression">Count regression</a></li>
<li><a href="#log-normal-model">Log-Normal model</a></li>
<li><a href="#generalized-linear-models">Generalized Linear Models</a>
<ul>
<li><a href="#bernoulli">Bernoulli</a></li>
<li><a href="#scaled-binomial">Scaled binomial</a></li>
<li><a href="#showing-that-a-distribution-is-a-member-of-the-exponential-family">Showing that a distribution is a member of the exponential family</a></li>
<li><a href="#maximum-likelihood-for-glms">Maximum Likelihood for GLMs</a></li>
<li><a href="#model-fit-and-model-choice">Model Fit and Model Choice</a></li>
</ul></li>
</ul></li>
<li><a href="#categorical-regression">3. Categorical regression</a></li>
<li><a href="#quantile-regression">4. Quantile regression</a></li>
<li><a href="#mixed-models">5. Mixed Models</a></li>
</ul>
</div>

<div id="introduction---roadmap-for-this-semester" class="section level1">
<h1>1. Introduction - Roadmap for this semester</h1>
<hr />
<p>What are Generalized Linear Models? Generalizations of the linear model,Duh!</p>
<p>Linear Model <span class="math inline">\(y = \mathbf x^\prime \boldsymbol\beta + \epsilon, \epsilon \sim \mathbb{N}(0,1)\)</span></p>
<p>What are we doing here?</p>
<ul>
<li>Explain <span class="math inline">\(y\)</span> as a function of <span class="math inline">\(\mathbf x\)</span>.</li>
<li>Assume a distribution for <span class="math inline">\(y\)</span>, conditional on <span class="math inline">\(\mathbf x\)</span> (Gaussian)</li>
<li>The role of <span class="math inline">\(\mathbf x\)</span> is to model the mean of the resulting Gaussian, i.e. <span class="math inline">\(y \sim \mathbb{N}(\mu(\mathbf x), \sigma^2)\)</span>, where e.g. <span class="math inline">\(\mu(\mathbf x) = \mathbf x^\prime \boldsymbol \beta\)</span></li>
</ul>
<p>What do we want to do in this course in the upcoming few months?</p>
<p><em>Relax assumption that</em></p>
<ol style="list-style-type: decimal">
<li><span class="math inline">\(y\)</span> is conditionally Gaussian. (Ch. 2, 3)</li>
<li><span class="math inline">\(x\)</span> only affects the mean of <span class="math inline">\(y\)</span> (explicitly in Ch. 4, but remember in case of e.g. Poisson distribution <span class="math inline">\(\mathbb{E}=\mathbb{Var}=\lambda\)</span>, which is the parameter we model).</li>
<li>Influence is linear in <span class="math inline">\(x\)</span> (Ch. 5, leave the realm of GLMs and open up tp GAMs and STAR).</li>
</ol>
<p>Mixed models (Ch.6) tackle the correlation structure (think of repeated measurements) and Duration time analysis (Ch.7) comes with a very different way of looking at a model. This will become clear in the end.</p>
<hr />
</div>
<div id="glm-for-non-normal-responses" class="section level1">
<h1>2. GLM for non-normal responses</h1>
<div id="binary-models" class="section level2">
<h2>Binary models</h2>
<p><img style="float: right;" src="images/01.jpg"></p>
<p><strong>Binary regression is classification!</strong></p>
<div id="linear-probability-model" class="section level3">
<h3>Linear probability model</h3>
<p><span class="math display">\[
\begin{aligned}
y_i =0 &amp;\rightarrow \epsilon_i = y_i - x_i^\prime \beta = - x_i^\prime \beta\\
y_i =1 &amp;\rightarrow \epsilon_i = 1- x_i^\prime\beta\\
\end{aligned}
\]</span> <span class="math inline">\(\rightarrow \epsilon_i\)</span> is discrete</p>
<p><span class="math display">\[
x_i^\prime \beta \in \mathbb{R} \rightarrow \hat{y}_i = x_i^\prime \hat{\beta} \in \mathbb{R}
\]</span></p>
<p>In a GLM:</p>
<p><span class="math display">\[
\begin{aligned}
E(y_i) &amp;= 1 \cdot P(y_i=1) + 0\cdot P(y_i=0) \\
       &amp;= P(y_i = 1) = \pi_i
\end{aligned}
\]</span> <span class="math inline">\(\rightarrow\)</span> Model is continuous.</p>
<p><span class="math display">\[
\begin{aligned}
h: \mathbb{R} &amp;\rightarrow [0,1]\\
   x_i^\prime \beta &amp;\rightarrow \pi_i = h(x_i^\prime \beta)
\end{aligned}
\]</span></p>
<hr />
</div>
<div id="linkresponse-functions" class="section level3">
<h3>Link/response functions</h3>
<p>Logistic link: <span class="math display">\[
\begin{aligned}
h(\eta) &amp;= \frac{\exp(\log(\frac{\pi}{1-\pi}))}{1+ \exp(\log(\frac{\pi}{1-\pi]}))} \\
&amp;= \frac{\frac{\pi}{1-\pi}}{\frac{1-\pi +\pi}{1-\pi}} = \frac{\pi}{1} = \pi
\end{aligned}
\]</span></p>
<pre class="r"><code>eta &lt;- seq(-4, 4, length=100)
qplot(eta, plogis(eta), geom=&quot;line&quot;,  main=&quot;Logistic link&quot;)</code></pre>
<p><img src="ch1_files/figure-html/unnamed-chunk-1-1.png" width="672" style="display: block; margin: auto;" /></p>
<p>Important concept and easy to be confused about: <strong>odds</strong> : <span class="math inline">\(\frac{\pi}{1-\pi}= \frac{\text{success prob.}}{\text{failure prob.}}\)</span></p>
<p>If odds</p>
<ul>
<li><span class="math inline">\(=1 \rightarrow \text{success and failure equally likely}\)</span></li>
<li><span class="math inline">\(&gt;1 \rightarrow \text{success more likely than failure}\)</span></li>
<li><span class="math inline">\(&lt;1 \rightarrow \text{success less likely than failure}\)</span></li>
</ul>
<p><strong>Interpretation relative:</strong> <span class="math inline">\(x_{j+1} / x_j\)</span> (to enable comparison)</p>
<p><span class="math display">\[
\frac{\frac{\pi(x_{j}+1)}{1-\pi(x_{j}+1)}}{\frac{\pi(x_{j})}{1-\pi(x_{j})}} = \exp(\beta_j)
\]</span></p>
<hr />
</div>
<div id="response-functions" class="section level3">
<h3>Response functions</h3>
<p>All response functions are cumulative distribution functions (cdfs).</p>
<pre class="r"><code>eta &lt;- seq(-4, 4, length=100)

pcll &lt;- function(x) 1-exp(-exp(x))

ggdat &lt;-tibble(&quot;eta&quot;=eta, &quot;logit&quot;=plogis(eta), &quot;probit&quot;=pnorm(eta), &quot;cll&quot;=pcll(eta) )
ggdat %&gt;% pivot_longer(c(&quot;logit&quot;, &quot;probit&quot;, &quot;cll&quot;), names_to=&quot;link&quot;) %&gt;% 
  ggplot + geom_line(aes(x=eta, y=value, colour=link)) + ggtitle(&quot;Response functions&quot;)</code></pre>
<p><img src="ch1_files/figure-html/unnamed-chunk-2-1.png" width="672" style="display: block; margin: auto;" /></p>
<p>Because <span class="math inline">\(h\)</span> are cdfs, naturally there is an associatted distribution (with expectation and variance).</p>
<ul>
<li><strong>logistic</strong>: <span class="math inline">\(\mathrm{E}(x) = 0, \mathrm{Var}(x) = \frac{\pi^2}{3}\)</span></li>
<li><strong>probit</strong>: <span class="math inline">\(\mathrm{E}(x) = 0, \mathrm{Var}(x) = 1\)</span></li>
<li><strong>cloglog</strong>: <span class="math inline">\(\mathrm{E}(x) = -0.5772, \mathrm{Var}(x) = \frac{\pi^2}{6}\)</span></li>
</ul>
<p>You can always standardize the links/effects to make them comparable.</p>
<pre class="r"><code>eta &lt;- seq(-4, 4, length=100)

ggdat &lt;-tibble(&quot;eta&quot;=eta, &quot;logit&quot;=plogis(eta*pi/sqrt(3)), &quot;probit&quot;=pnorm(eta), &quot;cll&quot;=1-exp(-exp(eta*pi/sqrt(6)-0.5772)) )
ggdat %&gt;% pivot_longer(c(&quot;logit&quot;, &quot;probit&quot;, &quot;cll&quot;), names_to=&quot;link&quot;) %&gt;% mutate(value = scale(value)) %&gt;%
  ggplot + geom_line(aes(x=eta, y=value, colour=link)) + ggtitle(&quot;Standardized response functions&quot;)</code></pre>
<p><img src="ch1_files/figure-html/unnamed-chunk-3-1.png" width="672" style="display: block; margin: auto;" /></p>
<ul>
<li>differences in link functions can be canceled to a large extent by linear operations in the predictor (linear operation on the scale of the predictor lead to nonlinear transformation on the scale of <span class="math inline">\(h\)</span>! (as cdfs are nonlinear with very few exceptions (<span class="math inline">\(\mathbb{U}\)</span>))</li>
<li>since a (GLM) does exactly that, influence of link functions is smaller than on the first glance</li>
</ul>
<hr />
</div>
<div id="latent-utility-model" class="section level3">
<h3>Latent utility model</h3>
<p>Why do we do this? It is just the general approach towards binary response models. Think the other way round: we started with links and stated later that they are cdfs. Now, we derive that they are cdfs (through <span class="math inline">\(\epsilon\)</span>) and then think about what kind of links they could represent. <!-- Binary responses $y_i \in \{0,1\}$ --> <!-- Utility differences $\tilde{y}_i = \boldsymbol{x}_i^\prime \boldsymbol{\beta} + \epsilon_{i}$ --> <!--  $$ --> <!-- \begin{aligned} --> <!--  P(y_i = 1) &= P(\text{"differences in utilities is positive"})  \\ --> <!--  &= P(\tilde{y}_i >0) \\ --> <!--  &= P(\boldsymbol{x}^\prime_i + \epsilon_i >0) --> <!-- \end{aligned} --> <!-- $$ --> <span class="math display">\[
\begin{aligned}
P(y_i = 1) &amp;= P(\tilde{y}_i &gt;0) \\
&amp;= 1- P(\tilde{y}_i \leq 0) \\
&amp; = 1-P(\boldsymbol{x}_i^\prime \boldsymbol{\beta} + \epsilon \leq 0)\\
&amp; = 1- P(\epsilon_i \leq - \boldsymbol x_i^\prime \boldsymbol \beta) \\
&amp;= 1- h(-\boldsymbol{x}_i^\prime \boldsymbol \beta)\\
&amp;= h(\boldsymbol {x}_i^\prime \boldsymbol \beta)
\end{aligned}
\]</span></p>
<p><span class="math inline">\(\rightarrow\)</span> <span class="math inline">\(h\)</span> corresponds to a symmetric distribution function <span class="math inline">\(F\)</span> (e.g. logit, probit), if <span class="math inline">\(F(\eta) = 1-F(-\eta)\)</span> and we can write the model in the general form <span class="math inline">\(\pi=F(\eta)\)</span> and <span class="math inline">\(\eta = F^{-1}(\pi)\)</span>.</p>
<p>If assymmetric (e.g cloglog): <span class="math inline">\(\eta = - F^{-1}(1-\pi)\)</span>.</p>
<hr />
</div>
<div id="interpretation-of-parameters-in-the-logit-model" class="section level3">
<h3>Interpretation of parameters in the logit model</h3>
<p>The influence of the explanatory variables on the probability <span class="math inline">\(\pi =P(y=1)\)</span> is nonlinear and quite obscure. <strong>Direct interpretability only via the signs of <span class="math inline">\(\beta\)</span></strong> ,i.e. if <span class="math inline">\(\beta&gt;0\)</span>, then <span class="math inline">\(\pi\)</span> increases and vice versa. We need the odds ratio for more exact interpretations.</p>
<p>Don’t be confused:</p>
<ul>
<li>“success” probability <span class="math inline">\(\pi=P(y=1)\)</span>, “failure” proability <span class="math inline">\(1-\pi = P(y=0)\)</span></li>
<li><strong>odds</strong> are defined as the “success” probability divided by the “failure” probability: <span class="math inline">\(\frac{\pi}{1-\pi}\)</span>, e.g. if the chance of me winning a game is <span class="math inline">\(\pi=0.75\)</span> and the chance of losing is <span class="math inline">\(1-\pi=0.25\)</span>, then the odds are <span class="math inline">\(0.75/0.25= 3\)</span> to <span class="math inline">\(1\)</span> that I will win the game. Therefore, note that <strong>odds are no probabilities</strong>, but a ratio of “successes” divided by the “failures” (you get a probability by dividing by “failures” + “successes”). Odds follow a multiplicative model <span class="math inline">\(\frac{\pi}{1-\pi} = \exp(\beta_0)\exp(x_1\beta_1)...\)</span></li>
<li><strong>odds ratio</strong> is a ratio of odds and since odds are ratios, odds ratios are ratios of ratios (sorry). <strong>odds ratios are different from odds</strong> and can help interpreting regression coefficients, because they “isolate” their respective impact. Thus, they indicate relationships between two different configurations, e.g. odds of being sick with treatment (<span class="math inline">\(x =1\)</span>) vs. odds of being sick without treatment (<span class="math inline">\(x=0\)</span>) <span class="math inline">\(\rightarrow\)</span> compare with slide 41/box 5.2from regression book.</li>
</ul>
<p><span class="math display">\[
\begin{aligned}
\text{Odds ratio} &amp;= \frac{\text{odds}_{\text{treatment}}}{\text{odds}_{\text{no treatment}}} = \frac{\frac{P(\text{sick}|\text{treatment})}{P(\text{not sick}|\text{treatment})}}{\frac{P(\text{sick}| \text{no treatment})}{P(\text{not sick}|\text{no treatment})}} = \exp(\beta) \\
\end{aligned}
\]</span> The odds ratio of variable <span class="math inline">\(x \in \{1=\text{treatment}, 0 = \text{no treatment}\}\)</span> is the respective change in odds, when variable <span class="math inline">\(x\)</span> changes (increases from, if <span class="math inline">\(x\)</span> is cont.) <span class="math inline">\(0\)</span> to <span class="math inline">\(1\)</span>: <span class="math display">\[
\begin{aligned}
\text{odds}_{\text{treatment}} = \exp(\beta)\cdot \text{odds}_{\text{no treatment}}
\end{aligned}
\]</span> This would still hold if there were other covariables as their impact would cancel in this ratio.</p>
</div>
<div id="grouped-data" class="section level3">
<h3>Grouped data</h3>
<p><span class="math display">\[
\begin{aligned}
y_g \sim \text{Bin}(\pi_g, n_g) \rightarrow &amp;\mathrm{E}(y_g) = n_g \pi_g \\  
                                          &amp;\mathrm{Var}(y_g) = n_g \pi_g (1-\pi_g)\\
\bar{y}_g \sim \text{Bin}(\pi_g, n_g)/n_g \rightarrow &amp; \mathrm{E}({y}_g / n_g) = \pi_g\\
                                                    &amp;\mathrm{Var}(\bar{y}_g) = \mathrm{Var}({y_g /n_g}) =\mathrm{Var}({y_g})/n_g^{2} = n_g/n_g^2 \pi_g (1-\pi_g)=\pi_g    (1-\pi_g)/n_g
\end{aligned}
\]</span></p>
</div>
<div id="overdispersion" class="section level3">
<h3>Overdispersion</h3>
<p>Dispersion means variability and refers to variance. Overdispersion is a situation, where the empirical variance exceeds the “theoretical” variance that is expected from the model. Happens very often. Becomes clearer with count regression.</p>
<p>How do positively correlated observations lead to overdispersion? In few words: because the independence assumption inherent in binomially distributed rvs is not given anymore. Let <span class="math inline">\(Y_i \sim \mathrm{Ber}(\pi)\)</span>, then, if <span class="math inline">\(Y_i\)</span> are independent</p>
<p><span class="math display">\[\begin{align}
Y &amp;=\sum_{i=1}^n Y_i \sim \mathrm{Bin}(n,\pi) \\
\mathbb{E}&amp;= n\pi &amp; \mathbb{Var}(Y) = n\pi(1-\pi) \label{eq:iidbin}\tag{1}
\end{align}\]</span></p>
<p>However, if there is correlation, the variance of a sum isn’t the sum of the variances anymore, i.e. <span class="math inline">\(\mathbb{Var}(\sum_{i=1}^n Y_i) \neq \sum_{i=1}^n \mathbb{Var}(Y_i)\)</span> which was used in eq. 1 (<span class="math inline">\(\sum_{i=1}^n \mathbb{Var}({Y_i}) =\sum_{i=1}^n \pi(1-\pi) = n\pi(1-\pi)\)</span> )</p>
<p>For correlated obs. <span class="math inline">\(Corr(Y_i, Y_j)=\rho &gt; 0\)</span> expectation is the same (invariant) <span class="math inline">\(\mathbb{E}= n\pi\)</span>, but the variance becomes <span class="math display">\[
\mathbb{Var}(Y)=\mathbb{Var}(\sum_{i=1}^n Y_i) = n\pi(1-\pi)(1 + \rho(n-1))
\]</span> which is certainly <span class="math inline">\(&gt; n\pi(1-\pi)\)</span>, the variance we would expect under the binomial model. (<span class="math inline">\(\rho &lt;0\)</span> would lead to underdispersion). Does the left term look familiar, maybe to the scaling factor <span class="math inline">\(\phi\)</span> on slide 49? Take a look at <a href="https://en.wikipedia.org/wiki/Variance#Sum_of_correlated_variables" class="uri">https://en.wikipedia.org/wiki/Variance#Sum_of_correlated_variables</a> if you want and try to derive it from there (just plug in our <span class="math inline">\(\mathbb{Var}\)</span> for <span class="math inline">\(\sigma^2\)</span>) Think of overdispersion not simply of “too much variance”. It means more variance than expected. There is a difference!</p>
</div>
<div id="maximum-likelihood-for-binary-glm" class="section level3">
<h3>Maximum Likelihood for binary GLM</h3>
<p>Log-likelihood <span class="math display">\[
\begin{aligned}
l(\boldsymbol{\beta}) &amp;= \log(L(\boldsymbol{\beta} \beta))\\
&amp;= \sum_{i=1}^{n}l_i(\boldsymbol{\beta})\\
&amp;= \sum_{i=1}^{n}y_i \log(\pi_i) + (1-y_i)\log(1-\pi_i)
\end{aligned}
\]</span> “Score contribution” (returns vector of length of <span class="math inline">\(\boldsymbol{\beta}\)</span>)</p>
<p><span class="math display">\[
\begin{aligned}
\boldsymbol{s}_i(\boldsymbol{\beta}) &amp;= \frac{\partial}{\partial \boldsymbol{\beta}} l_i (\boldsymbol{\beta})\\
&amp;= \frac{\partial}{\partial \boldsymbol{\beta}} \frac{\partial \eta_i}{\partial \eta_i} \frac{\partial \pi_i}{\partial \pi_i} l_i(\boldsymbol{\beta}) \text{   (chain rule)}\\
&amp;= \frac{\partial \eta_i}{\partial \boldsymbol{\beta}} \frac{\partial \pi_i}{\partial \eta_i} \frac{\partial}{\pi_i} l_i(\boldsymbol{\beta})
\end{aligned}
\]</span> Score function is the sum of score contributions (as with the log-likelihood) <span class="math display">\[
\begin{aligned}
\boldsymbol{s}(\boldsymbol{\beta}) = \sum_{i=1}^{n}\boldsymbol{s}_i(\boldsymbol{\beta})
\end{aligned}
\]</span></p>
<p>Newton-Raphson</p>
<p><img src="images/animation.gif" /></p>
</div>
</div>
<div id="count-regression" class="section level2">
<h2>Count regression</h2>
<p><img style="float: right;" src="images/CountVonCount.png"></p>
<p>In general, similar issues with <span class="math inline">\(\boldsymbol{x}_i^\prime \boldsymbol{\beta}\)</span> as before (not discrete and positivity not guaranteed), i.e.</p>
<ul>
<li><span class="math inline">\(y_i\)</span> discrete but <span class="math inline">\(\boldsymbol{x}_i^\prime \boldsymbol{\beta} \text{ continuous.}\)</span></li>
<li><span class="math inline">\(y_i \geq 0\)</span> but <span class="math inline">\(\boldsymbol{x}_i^\prime \boldsymbol{\beta} \in \mathbb{R}\)</span></li>
</ul>
<p>Why not try a similar idea as in logistic regression? That is, assume a distribution for <span class="math inline">\(y_i\)</span> and model <span class="math inline">\(\mathrm{E}(y_i) = h(\boldsymbol{x}_i^\prime \boldsymbol{\beta})\)</span>. This idea, i.e. assuming a distribution for <span class="math inline">\(y_i\)</span> and model the respective parameter(s) by transforming a linear predictor <span class="math inline">\(\boldsymbol{x}_i^\prime \boldsymbol{\beta}\)</span> such that it fulfills the respective properties (e.g. positivity for <span class="math inline">\(\lambda\)</span> or {0,1} for <span class="math inline">\(\pi\)</span> etc.) is applied to many problems in GLMs.</p>
</div>
<div id="log-normal-model" class="section level2">
<h2>Log-Normal model</h2>
<p>What kind of random variable is log-normal distributed? An rv that is normal, when logarithmized. Note that <span class="math display">\[ \mathrm{E}(y_i) \neq \exp(\mathrm{E}(\tilde{y}_i)), \text{ where } \tilde{y}_i=\log(y_i)\]</span> In other words, this can result in (sometimes huge) bias! Furthermore, this means that the log-normal model is not a GLM in definition of the next section!</p>
<p>   </p>
</div>
<div id="generalized-linear-models" class="section level2">
<h2>Generalized Linear Models</h2>
<p><strong>Exponential family</strong> Why is it important? Because if we can show that a response distribution belongs to the exponential family, we immediately know its inferential properties and can present it in a unified framework. Probably most of the distributions you have worked with so far belong to the exponential family.</p>
<p><img style="float: center;" src="images/tab_expf.png"></p>
<p>(Fahrmeir, Ludwig, et al. “Regression models.” Regression. Springer, Berlin, Heidelberg, 2013. 303) <strong>Table shows the canonical links!</strong></p>
<ul>
<li><span class="math inline">\(\theta\)</span> is called the canonical parameter; think of what we do here:
<ol style="list-style-type: decimal">
<li><p>specify linear predictor <span class="math inline">\(\eta = \boldsymbol{x}^\prime \boldsymbol{\beta}\)</span> to model the expectation <span class="math inline">\(\mu\)</span> via the link function <span class="math inline">\(g\)</span>: <span class="math inline">\(\mu=g^{-1}(\eta)\)</span> or <span class="math inline">\(g(\mu) = \eta\)</span></p></li>
<li><p>often <span class="math inline">\(\mu\)</span> cannot be <span class="math inline">\(=\boldsymbol{x}^\prime\boldsymbol{\beta}\)</span>,but requires to be positive, between zero and one etc. so, we need to transform <span class="math inline">\(\eta\)</span> in a function <span class="math inline">\(\mu (\boldsymbol{x})\)</span></p></li>
<li><p><span class="math inline">\(\theta\)</span> is defined on the scale of <span class="math inline">\(y\)</span> and needs to be retransformed via <span class="math inline">\(\theta = (b^\prime)^{-1}(\mu)\)</span> see below;<br />
</p>
<p><strong>If the link function relates <span class="math inline">\(\theta\)</span>, <span class="math inline">\(\mu\)</span> and <span class="math inline">\(\eta\)</span> such that <span class="math inline">\(\eta=\theta\)</span>, we call it the canonical link, given by <span class="math inline">\(g=(b^\prime)^{-1}\)</span></strong>. <span class="math inline">\(b\)</span> is related to the distribution and governs what is canonical.<br />
</p></li>
</ol></li>
<li>every exponential family member has a canonical link function, resulting in <span class="math inline">\(\theta_i = \eta_i = \boldsymbol{x}_i^\prime \boldsymbol{\beta}\)</span> E.g. for Bernoulli, it is the logistic link function, <strong>not probit or cll</strong>! The canonical link guarantees properties that guarantee a smooth estimation process.</li>
<li>function <span class="math inline">\(b()\)</span> becomes clear when brought into the exponential family form; enters expectation + variance via</li>
</ul>
<p><span class="math display">\[\begin{align}
\mathbb{E}(y) &amp;= \mu= b^\prime(\theta)  &amp; \mathbb{Var}(y) =\phi b^{\prime\prime}(\theta)
\end{align}\]</span></p>
<ul>
<li><span class="math inline">\(\phi\)</span> is dispersion parameter; often assumed to be known</li>
<li><span class="math inline">\(c()\)</span> is a normalizing constant, independent of <span class="math inline">\(\theta\)</span></li>
<li><span class="math inline">\(w\)</span> is a weight</li>
</ul>
<p>A GLM is fully defined by</p>
<ol style="list-style-type: decimal">
<li>the <strong>random component</strong> specified in terms of the conditional exponential family density <span class="math inline">\(f(y|\cdot)\)</span></li>
<li>the <strong>non-random component</strong> in form of the linear predictor <span class="math inline">\(\boldsymbol{x}^\prime\boldsymbol{\beta}\)</span></li>
<li>the <strong>link function</strong> <span class="math inline">\(g(\mu)=\eta=\boldsymbol{x}^\prime \boldsymbol{\beta}\)</span> or <span class="math inline">\(\mu = g^{-1}(\eta)\)</span>; maps to required space, see above</li>
</ol>
<div id="bernoulli" class="section level3">
<h3>Bernoulli</h3>
<p><span class="math display">\[
\begin{aligned}
\log(f(y_i|\pi_i)) &amp;= y_i \log(\pi_i) - y_i \log(1-\pi_i) + \log(1-\pi) \\
&amp;= y_i \log(\frac{\pi_i}{1-\pi_i}) + \log(1-\pi_i),
\end{aligned}
\]</span> where</p>
<ul>
<li><span class="math inline">\(\log(\frac{\pi_i}{1-\pi_i}) = \theta_i\)</span></li>
<li><span class="math inline">\(\log(1-\pi)=\log(\frac{1}{1+\exp(\theta_i)})=-\log(1+\exp(\theta_i)) = - b(\theta_i)\)</span></li>
</ul>
<p>Connection to expectation and variance: <span class="math display">\[
\begin{aligned}
b^\prime(\theta_i) &amp;= \frac{\partial}{\partial \theta_i} \log(1+\exp(\theta_i))\\
&amp;= \frac{1}{1+\exp(\theta_i)} \exp(\theta_i)\\
&amp;= \pi_i =\mathrm{E}_i\\
b^{\prime\prime}(\theta_i) &amp;= \frac{\partial}{\partial \theta_i} \frac{\exp(\theta_i)}{1+\exp(\theta_i)}\\
&amp;= \frac{\exp(\theta_i)(1+\exp(\theta_i)) - \exp(\theta_i)\exp(\theta_i)}{(1+\exp(\theta_i))^2}\\
&amp;=\frac{\exp(\theta_i)}{1+\exp(\theta_i)} \frac{1}{1+\exp(\theta_i)}\\
&amp;= \pi_i(1-\pi_i) = \mathbb{Var}
\end{aligned}
\]</span></p>
</div>
<div id="scaled-binomial" class="section level3">
<h3>Scaled binomial</h3>
<p><span class="math display">\[
\begin{aligned}
\log(f(\bar{y}_i)) &amp;= n_i \bar{y}_i \log(\pi_i) + (n_i -n_i \bar{y})\log(1-\pi_i) + {\log{n_i} \choose {n_i\bar{y}_i}}\\
&amp;= n_i \bar{y}_i \log \frac{\pi_i}{1-\pi_i} + n_i\log(1-\pi) + \log {n_i \choose n_i \bar{y}_i},
\end{aligned}
\]</span> where</p>
<ul>
<li><span class="math inline">\(n_i \bar{y}_i \log \frac{\pi_i}{1-\pi_i} = \theta_i\)</span></li>
<li><span class="math inline">\(n_i\log(1-\pi) = - \log(1+\exp(\theta_i))\)</span></li>
</ul>
</div>
<div id="showing-that-a-distribution-is-a-member-of-the-exponential-family" class="section level3">
<h3>Showing that a distribution is a member of the exponential family</h3>
<p>There is no clear step-by-step manual, but the following approach often makes it easier to see what is what: <strong>Take the exp of the log of the given density you want to associate with the exp. family.</strong></p>
<p>I.e.,e.g. for Bernoulli:</p>
<span class="math display">\[\begin{aligned}
p(y|\pi) &amp;= \pi^y(1-\pi)^{1-y} &amp;\text{ oh, no this looks nothing like the exp family form}\\
&amp;= \exp \left( \log \left(\frac{\pi}{1-\pi}\right)y + \log(1-\pi)\right) &amp;\text{ much better!}\\
\end{aligned}\]</span>
<p>This works often, because the exp. family form is an <span class="math inline">\(\exp\)</span> of a sum/difference, which results from taking the <span class="math inline">\(\log\)</span>. What’s left is that you have to be really sure about what is linear in which parameter and especially what term can contain what (e.g. that the normalizing constant cannot contain <span class="math inline">\(\theta\)</span> etc.)</p>
</div>
<div id="maximum-likelihood-for-glms" class="section level3">
<h3>Maximum Likelihood for GLMs</h3>
<p>Advantages of the canonical link:</p>
<ul>
<li>always concave log-likelihood leading to unique ML estimators</li>
<li><span class="math inline">\(\boldsymbol{F}(\boldsymbol{\beta}) = \boldsymbol{H}(\boldsymbol{\beta})\)</span></li>
</ul>
<p><span class="math display">\[
\begin{aligned}
s(\boldsymbol{\beta}) &amp;= \sum_{i=1}^n s_i(\boldsymbol{\beta}) \\
s_i(\boldsymbol{\beta}) &amp;= \frac{\partial}{\partial \boldsymbol{\beta}} l_i(\boldsymbol{\beta}) \\
l_i(\boldsymbol{\beta}) &amp;= \frac{y_i \theta_i - b(\theta_i)}{\phi}\omega_i + c(y_i, \phi, \omega_i)\\
s_i(\boldsymbol{\beta}) &amp;= \frac{\partial\eta_i}{\partial \boldsymbol{\beta}} \frac{\partial \mu_i}{\partial \eta_i} \frac{\partial \theta_i}{\partial \mu_i} \frac{\partial}{\partial \theta_i} l_i(\boldsymbol{\beta}) &amp; (2)\\ 
\end{aligned}
\]</span> What do those terms mean?</p>
<ul>
<li><span class="math inline">\(\frac{\partial\eta_i}{\partial \boldsymbol{\beta}} = \frac{\partial}{\partial \boldsymbol{\beta}} \boldsymbol{x}_i^\prime \boldsymbol{\beta}= \boldsymbol{x}_i\)</span></li>
<li><span class="math inline">\(\frac{\partial \mu_i}{\partial \eta_i} =\frac{\partial}{\partial \eta_i} h(\eta_i) = h^\prime(\eta_i)\)</span></li>
<li><span class="math inline">\(\frac{\partial \theta_i}{\partial \mu_i} = \left(\frac{\partial \mu_i}{\partial \theta_i} \right)^{-1} = \left(\frac{\partial b^\prime(\theta_i)}{\partial \theta_i}\right)^{-1} = (b^{\prime\prime}(\theta_i))^{-1} = \frac{1}{b^{\prime\prime}(\theta_i)}\)</span></li>
<li><span class="math inline">\(\frac{\partial}{\partial \theta_i} l_i(\boldsymbol{\beta}) = (y_i - b^\prime(\theta_i))\frac{\omega_i}{\phi}, \text{ where } b^\prime(\theta_i) =\mu_i\)</span></li>
</ul>
<p>Plug into eq. (2):</p>
<p><span class="math display">\[
s_i(\boldsymbol{\beta}) = \boldsymbol{x}_i \frac{h^\prime (\eta_i)}{b^{\prime\prime}(\theta_i)} \frac{\phi}{\omega_i}(y_i - \mu_i)
\]</span></p>
<div id="expected-fisher-information" class="section level4">
<h4>Expected Fisher Information</h4>
<p><span class="math display">\[\begin{align}
\boldsymbol{F}(\boldsymbol{\beta}) &amp;= \mathbb{E}(\boldsymbol{F}^\star(\boldsymbol{\beta}))\\
&amp;= -\frac{\partial}{\partial\boldsymbol{\beta}} \boldsymbol{s}(\boldsymbol{\beta})\\
&amp;= \text{Cov}(\boldsymbol{s}(\boldsymbol{\beta}))\\
&amp;=E(\boldsymbol{s(\boldsymbol{\beta})}s(\boldsymbol{\beta})^\prime)\\
&amp;=\sum_{i=1}^n \mathbb{E}(\boldsymbol{s}_i(\boldsymbol{\beta})s_i(\boldsymbol{\beta})^\prime)\\
&amp;=\sum_{i=1}^n \boldsymbol{F}_i(\boldsymbol{\beta})\\
\boldsymbol{F}_i(\boldsymbol{\beta}) &amp;= \boldsymbol{x}_i\left(\frac{h^\prime(\eta_i)}{\sigma_i^2}\right)^2\mathbb{E}((y_i - \mu_i)^2)\boldsymbol{x}_i^\prime\\
&amp;= \boldsymbol{x}_i\boldsymbol{x}_i^\prime \frac{(h^\prime(\eta_i))^2}{\sigma_i^2}\\
\end{align}\]</span></p>
</div>
</div>
<div id="model-fit-and-model-choice" class="section level3">
<h3>Model Fit and Model Choice</h3>
<div id="pearson-statistic" class="section level4">
<h4>Pearson statistic</h4>
<p><span class="math display">\[\chi^2 = \sum_{i=1}^G \frac{(y_i-\hat{\mu})^2}{v(\hat{\mu_i})/w_i}\]</span></p>
</div>
<div id="deviance" class="section level4">
<h4>Deviance</h4>
<p><span class="math display">\[D = - 2 \sum_{i=1}^G(l_g(\hat{\mu}_g) - l_g(\bar{y}))\]</span> Both statistics are approx. <span class="math inline">\(\sim \phi \chi^2(G-p)\)</span>, where <span class="math inline">\(p\)</span> is the number of parameters and <span class="math inline">\(G\)</span> is the number of groups. The corresponding test of model fit compares the estimated model fit to that of the saturated mode. We assume that under <span class="math inline">\(H_0\)</span> the estimated (not the saturated) model is true.</p>
<p>Important concepts:</p>
<ul>
<li>Saturated model: a model very <strong>every</strong> observation has its own parameter estimated, resulting in a “perfect” fit; we need this concept as a benchmark of good model fit, we can compare our models to (e.g in Pearsons statistic and deviance).</li>
<li><strong>perfect model fit does not mean perfect or even good model (quite the contrary)</strong> Its just says that our model is very adapted to the data (in-sample). Pearson an deviance are very naive approaches for evaluating a model, as they only capture this kind of adaption. Selection criteria such as the AIC are also in-sample, but try to penalize too high adaptivity.</li>
<li>“Too good” a model fit leads to overfitting, take a look at <img src="images/overfitting.png" /> (Bishop, Christopher M. Pattern recognition and machine learning. springer, 2006.) Which model fits better to the data? Which model to you think has better predictive performance? Will be treated in much more detail later. Just remember that it is very easy to construct a model that interpolates your data, but predicts really badly.</li>
</ul>
</div>
</div>
</div>
</div>
<div id="categorical-regression" class="section level1">
<h1>3. Categorical regression</h1>
<p><strong>GOAL: Estimate probabilities e.g. “What is the probability that person i has infection type r?”</strong></p>
<p><span class="math display">\[\pi_{ir} = P(Y_i = r)=P(y_{ir}=1)\]</span></p>
</div>
<div id="quantile-regression" class="section level1">
<h1>4. Quantile regression</h1>
<p>In linear regression, we want to model the conditional mean <span class="math inline">\(E(y|x) =x \beta\)</span>. This worked by minimizing the residual sum of squares <span class="math inline">\(E((y-\mu)^2|x)\)</span> by assuming <span class="math inline">\(\mu=x\beta\)</span>. BUT modelling, e.g. the median <span class="math inline">\(x_{med} = x\beta\)</span> could be even more interesting. In this case we minimize <span class="math inline">\(E(|y-x_{med}||x)\)</span> by assuming <span class="math inline">\(x_{med} = x\beta_{med}\)</span>. But why stop at the median? In general, we may want to establish a relationship between the covariates and quantiles of response distribution <span class="math display">\[
q_\tau(y|x) = x\beta_\tau
\]</span></p>
<p>That is the goal of quantile regression. In comparison to e.g. normal regression, we can model the whole distribution of the response in conjunction with the covariates without assuming a fixed distribution (as in e.g. normal, poisson … regression; a distribution is fully caputured by its quantiles if you align them close enough e.g. <span class="math inline">\(\tau \in (0.05, 0.1, ..., 0.95, 1.0))\)</span>. Think of CONDITIONAL quantiles, i.e. condition on a covariate value (usually a set of values) and look at the quantiles of the resulting distribution of <span class="math inline">\(y\)</span>.</p>
<p><img src="images/qr_munich.png" /></p>
</div>
<div id="mixed-models" class="section level1">
<h1>5. Mixed Models</h1>
<p>Intra-class correlation coefficient for random intercept model: [ (y_{ij}, y_{ik}) =  ] <span class="math display">\[\begin{align}
Var(y_{ij}) = Var(y_{ik})&amp;= Var(\boldsymbol{x}^T\boldsymbol{\beta}+ b_i + \epsilon_{ij})\\
&amp;= Var(b_i + \epsilon_{ij}) &amp;\text{$\boldsymbol{\beta}$ is not random!}\\
&amp;= Var(b_i) + Var(\epsilon_{ij}) + 2Cov(b_i + \epsilon_{ij})\\
&amp;= \sigma^2 + \tau^2\\
\end{align}\]</span></p>
<p><span class="math display">\[\begin{align}
Cov(y_{ij},y_{ik}) &amp;= Cov(\boldsymbol{x}^T\boldsymbol{\beta}+ b_i + \epsilon_{ij}, \boldsymbol{x}^T\boldsymbol{\beta}+ b_i + \epsilon_{ik}) \\
&amp;= Cov(b_i +\epsilon_{ik}, b_i + \epsilon_{ik})\\
&amp;= Cov(b_i, b_i) + Cov(b_i, \epsilon_{ik}) + Cov(\epsilon_{ik}, b_i) + Cov(\epsilon_{ij}, \epsilon_{ik})\\
&amp;= \tau^2
\end{align}\]</span></p>
<p>[ (y_{ij}, y_{ik}) =  =  ]</p>
<p>Think of how you would derive the intra-class correlation coefficient for the random slopes model!</p>
<p>Note that both the random intercept and the random slope model assume that the covariate effects are the same for each individual or cluster.</p>
</div>




</div>

<script>

// add bootstrap table styles to pandoc tables
function bootstrapStylePandocTables() {
  $('tr.odd').parent('tbody').parent('table').addClass('table table-condensed');
}
$(document).ready(function () {
  bootstrapStylePandocTables();
});


</script>

<!-- tabsets -->

<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});

$(document).ready(function () {
  $('.tabset-dropdown > .nav-tabs > li').click(function () {
    $(this).parent().toggleClass('nav-tabs-open');
  });
});
</script>

<!-- code folding -->


<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
